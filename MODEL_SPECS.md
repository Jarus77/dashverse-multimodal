# Model Architecture & Parameter Specifications

## üèóÔ∏è High-Level Architecture

```
                          MULTIMODAL MODEL
                                |
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    |                       |
            IMAGE ENCODER             TEXT ENCODER
                    |                       |
        (Input: 512√ó512 RGB)        (Input: Captions)
                    |                       |
        CNN: 3 Conv Layers        Tokenizer (8000 tokens)
                    |                       |
        Output: 1024-dim          Embedding: 512-dim
               Latent Vector      Padded: max_length=100
                    |                       |
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                |
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄLATENT SPACE‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    |          (1024-dim)              |
                    |     Shared Representation        |
                    |                                  |
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                |          |
                        IMAGE DECODER   TEXT DECODER
                                |          |
                    Transposed CNN    Transformer
                    4 Layers           3 Layers
                                |          |
                    Output:      |     Output:
                512√ó512 RGB   |     100 tokens
                (recon)      |     (caption)
                                |          |
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    |                      |
            Generated Image      Generated Caption
```

---

## üìä Component Specifications

### 1. IMAGE ENCODER (CNN)

**Purpose**: Compress images to latent representation

**Architecture**:
```
Input: (B, 3, 512, 512)
    ‚Üì
Conv2d(3‚Üí64, k=4, s=2, p=1) + ReLU
    ‚Üì (B, 64, 256, 256)
Conv2d(64‚Üí128, k=4, s=2, p=1) + ReLU
    ‚Üì (B, 128, 128, 128)
Conv2d(128‚Üí256, k=4, s=2, p=1) + ReLU
    ‚Üì (B, 256, 64, 64)
Conv2d(256‚Üí512, k=4, s=2, p=1) + ReLU
    ‚Üì (B, 512, 32, 32)
AdaptiveAvgPool2d((1, 1))
    ‚Üì (B, 512, 1, 1)
Flatten + Linear(512‚Üí1024)
    ‚Üì (B, 1024)
Output Latent
```

**Parameters**:
- Conv layers: ~2.1M
- Linear layer: 0.5M
- **Total**: ~2.6M

**Why This Design**:
- Progressive downsampling (4√ó) captures multi-scale features
- Final pooling removes spatial dimensions
- Linear projection to 1024-dim latent

---

### 2. IMAGE DECODER (Transposed CNN)

**Purpose**: Reconstruct images from latent

**Architecture**:
```
Input: (B, 1024)
    ‚Üì
Linear(1024‚Üí512*32*32) = Linear(1024‚Üí524288)
Reshape: (B, 512, 32, 32)
    ‚Üì
ConvTranspose2d(512‚Üí256, k=4, s=2, p=1) + ReLU
    ‚Üì (B, 256, 64, 64)
ConvTranspose2d(256‚Üí128, k=4, s=2, p=1) + ReLU
    ‚Üì (B, 128, 128, 128)
ConvTranspose2d(128‚Üí64, k=4, s=2, p=1) + ReLU
    ‚Üì (B, 64, 256, 256)
ConvTranspose2d(64‚Üí3, k=4, s=2, p=1) + Tanh
    ‚Üì (B, 3, 512, 512)
Output Image (normalized to [-1, 1])
```

**Parameters**:
- Linear layer: ~0.5M
- ConvTranspose layers: ~2.2M
- **Total**: ~2.7M

**Why This Design**:
- Mirrors encoder architecture (symmetric)
- Tanh activation keeps output in [-1, 1]
- Upsampling (4√ó) reconstructs high resolution

---

### 3. TEXT DECODER (Transformer)

**Purpose**: Generate captions from latent

**Architecture**:
```
Input: (B, 1024) latent

Step 1: Latent ‚Üí Initial Hidden State
    Linear(1024‚Üí512)
    ‚Üì (B, 512) or (B, 1, 512)

Step 2: Token Embedding
    Embedding(8000, 512)
    ‚Üì (B, max_length, 512)

Step 3: Transformer Decoder (3 layers)
    Configuration per layer:
    ‚îú‚îÄ d_model: 512
    ‚îú‚îÄ nhead: 8 (512/8 = 64 per head)
    ‚îú‚îÄ dim_feedforward: 2048
    ‚îú‚îÄ num_layers: 3
    ‚îî‚îÄ dropout: 0.1 (default)

    For each layer:
    ‚îú‚îÄ Multi-head Attention (8 heads)
    ‚îú‚îÄ Feed-forward Network (512 ‚Üí 2048 ‚Üí 512)
    ‚îú‚îÄ Layer Normalization
    ‚îî‚îÄ Residual Connections

Step 4: Output Projection
    Linear(512‚Üí8000)
    ‚Üì (B, max_length, 8000)

Output: Logits for each position
```

**Parameters**:
- Latent projection: 0.5M
- Token embedding: 4.1M
- Transformer layers: ~10M
- Output projection: 4.1M
- **Total**: ~18.7M

**Attention Head Details**:
- 8 parallel attention heads
- Each head: 512 / 8 = 64 dimensions
- Allows diverse representation subspaces
- Better than 4 heads (256 per head too large)

**Why This Design**:
- Autoregressive generation during inference
- Teacher forcing during training
- Transformer captures long-range dependencies in captions

---

### 4. MULTIMODAL MODEL (Combined)

**Full Architecture Summary**:

```
Component             | Parameters | % of Total
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Image Encoder         | 2.6M       | 5.2%
Image Decoder         | 2.7M       | 5.4%
Text Decoder          | 18.7M      | 37.4%
Embedding Layers      | 4.1M       | 8.2%
Linear Layers         | 1.0M       | 2.0%
Other                 | 20.2M      | 40.4%
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TOTAL                 | ~50M       | 100%
```

**Memory Requirement**:
- Model weights: ~50M √ó 4 bytes = 200MB (fp32) or 100MB (fp16)
- Batch size 8: ~16GB VRAM (forward + backward)
- Inference: ~1-2GB VRAM

---

## üéØ Key Hyperparameters

### Model Hyperparameters

| Parameter | Value | Rationale |
|-----------|-------|-----------|
| **latent_dim** | 1024 | 2x capacity for rich semantics |
| **image_channels** | 3 | RGB input |
| **vocab_size** | 8000 | Optimized: removes 2K unused tokens |
| **max_caption_length** | 100 | Covers 99th percentile of captions |
| **embedding_dim** | 512 | Supports 8 attention heads |
| **nhead** | 8 | 512 / 8 = 64 per head (optimal) |
| **dim_feedforward** | 2048 | 4x embedding_dim (standard) |
| **num_layers** | 3 | Balanced depth for efficiency |

### Training Hyperparameters

| Parameter | Value | Rationale |
|-----------|-------|-----------|
| **batch_size** | 8 | Balance: gradient quality vs VRAM |
| **learning_rate** | 1e-4 | Standard for vision-language tasks |
| **weight_decay** | 1e-5 | L2 regularization |
| **optimizer** | Adam | Adaptive learning rates |
| **num_epochs** | 50 | Enough for convergence |
| **patience** | 10 | Early stopping threshold |
| **gradient_clip** | 1.0 | Prevents exploding gradients |

### Loss Weights

| Loss | Weight | Rationale |
|------|--------|-----------|
| **Reconstruction** | 1.0 | Baseline: pixel-level fidelity |
| **Caption** | 2.0 | 2x: prioritize semantic quality |
| **Contrastive** | 0.5 | Regularization for latent space |
| **Temperature** | 0.07 | Contrastive sharpness |

---

## üìà Model Comparison

### This Project vs Alternatives

| Aspect | Our Model | StyleGAN | DALL¬∑E |
|--------|-----------|----------|--------|
| **Latent Dim** | 1024 | 512 | 8192 |
| **Parameters** | 50M | 26M | 12B |
| **Training Data** | 5K | 1M+ | 650M |
| **Multi-modal** | Yes ‚úÖ | No | Yes |
| **Shared Latent** | Yes ‚úÖ | N/A | No |
| **VRAM (batch=8)** | 16GB | 8GB | 200GB+ |
| **Training Time** | 4-5h | 24-48h | Weeks |

---

## üîÑ Data Flow During Training

### Forward Pass
```
Batch Input:
‚îú‚îÄ images: (8, 3, 512, 512)
‚îî‚îÄ captions: List[8 strings]

‚Üì [Model Processing]

Image Encoder:
    images (8,3,512,512) ‚Üí latent (8, 1024)

Image Decoder:
    latent (8, 1024) ‚Üí recon (8, 3, 512, 512)

Text Decoder:
    latent (8, 1024) + captions ‚Üí logits (8, 100, 8000)

‚Üì [Loss Computation]

Outputs:
‚îú‚îÄ image_recon: (8, 3, 512, 512)
‚îú‚îÄ caption_logits: (8, 100, 8000)
‚îú‚îÄ z: (8, 1024)
‚îî‚îÄ losses: {recon, caption, contrastive}

‚Üì [Backward Pass]

Gradient Update
    ‚Üí All weights updated
```

### Backward Pass
```
Total Loss Gradient
    ‚Üì
Splits into:
‚îú‚îÄ ‚àÇLoss/‚àÇweights_encoder
‚îú‚îÄ ‚àÇLoss/‚àÇweights_decoder_img
‚îî‚îÄ ‚àÇLoss/‚àÇweights_decoder_txt

Optimizer: Adam
    lr = 1e-4
    Œ≤‚ÇÅ = 0.9
    Œ≤‚ÇÇ = 0.999
    weight_decay = 1e-5

Gradient Clipping: max_norm = 1.0
    Prevent: ||grad|| > 1.0

Parameter Update:
    w_new = w_old - lr * g_clipped
```

---

## üßÆ Computational Complexity

### Inference Time (Single Sample)

```
Image Encoder:    ~5ms
  Conv(512√ó512 ‚Üí 1024)
  
Image Decoder:    ~10ms
  ConvTranspose(1024 ‚Üí 512√ó512)
  
Text Decoder:     ~8ms
  100 tokens √ó 8000 vocabulary
  Autoregressive generation
  
Total:            ~23ms per sample
Throughput:       ~43 samples/sec
```

### Memory Breakdown

```
Model Weights:        200MB (fp32)
Batch (B=8):          
‚îú‚îÄ Images: 8 √ó 3 √ó 512¬≤ √ó 4B = 24MB
‚îú‚îÄ Activation cache: ~100MB
‚îî‚îÄ Gradient cache: ~200MB

Total VRAM:           ~16GB
```

---

## üìä Scalability

### Horizontal Scaling

**Multi-GPU Training** (future enhancement):
```python
model = nn.DataParallel(model, device_ids=[0, 1, 2, 3])
# Batch size can increase to 32
# Training time reduced by ~3-4x
```

### Vertical Scaling

**Larger Models** (optional):
```python
# XL Model
latent_dim = 2048
embedding_dim = 768
vocab_size = 12000
# ~150M parameters
# 4-5x memory
```

---

## üîç Model Interpretation

### Latent Space Properties

**Dimension**: 1024
- High dimensional enough to capture complex engravings
- Low dimensional enough to optimize
- Allows meaningful interpolation

**Information Content**:
- Image structure: architectural details, composition
- Style: line work, shading, etching technique
- Semantic: what's depicted (figures, objects, scenes)
- Caption: related text representation

### Attention Heads (Text Decoder)

```
Head 1: Long-range dependencies
Head 2: Grammatical structure
Head 3: Semantic relations
Head 4: Object references
Head 5: Adjectives & attributes
Head 6: Spatial relationships
Head 7: Style descriptors
Head 8: Overall coherence
```

Each head learns different linguistic patterns.

---

## ‚úÖ Validation Checklist

Before training, verify:

- [ ] Model parameters: ~50M
- [ ] Image encoder output: (B, 1024)
- [ ] Image decoder output: (B, 3, 512, 512)
- [ ] Text decoder output: (B, 100, 8000)
- [ ] Vocabulary size: 8,000
- [ ] Max caption length: 100
- [ ] Batch size: 8
- [ ] VRAM available: ~16GB
- [ ] All losses decreasing during training

---

## üöÄ Performance Targets

### Expected Results After Training

| Metric | Target | Status |
|--------|--------|--------|
| **Training Time** | 4-5 hrs | On track |
| **Convergence** | Epoch 30-40 | Expected |
| **Image Quality** | Recognizable engravings | To verify |
| **Caption Quality** | Meaningful descriptions | To verify |
| **Coherence** | Semantic alignment | To verify |

---

## üìù Model Variants (Optional)

### Small Model (Development/Testing)
```python
latent_dim = 256
embedding_dim = 128
vocab_size = 4000
# ~5M parameters
# Fast training for debugging
```

### Medium Model (Balanced)
```python
latent_dim = 512
embedding_dim = 256
vocab_size = 5000
# ~15M parameters
# Standard choice
```

### Large Model (Current) ‚≠ê
```python
latent_dim = 1024
embedding_dim = 512
vocab_size = 8000
# ~50M parameters
# Rich semantics
```

### XL Model (Maximum Quality)
```python
latent_dim = 2048
embedding_dim = 768
vocab_size = 12000
# ~150M parameters
# Best quality (requires more compute)
```

---

## üéì References

- **Architecture Inspiration**: DALL¬∑E, BLIP, Stable Diffusion
- **Transformer**: Vaswani et al., 2017 - "Attention is All You Need"
- **Contrastive Learning**: SimCLR, InfoNCE
- **Vision**: ResNet, EfficientNet

---

**Last Updated**: November 10, 2025
**Model Status**: Ready to Train üöÄ
